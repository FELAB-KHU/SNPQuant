{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = 1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from peft import PeftModel  # 0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d050462c114c608ee85f560de40cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/indi/.local/share/virtualenvs/finance-python-GZ7M0BLx/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/indi/.local/share/virtualenvs/finance-python-GZ7M0BLx/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "base_model = \"NousResearch/Llama-2-13b-hf\"\n",
    "peft_model = \"FinGPT/fingpt-sentiment_llama2-13b_lora\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, trust_remote_code=True, device_map = \"cuda:0\", load_in_8bit = True,)\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " positive</s>\n",
      " neutral</s>\n",
      " negative</s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Make prompts\n",
    "prompt = [\n",
    "'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "Input: FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is aggressively pursuing its growth strategy by increasingly focusing on technologically more demanding HDI printed circuit boards PCBs .\n",
    "Answer: ''',\n",
    "'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "Input: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
    "Answer: ''',\n",
    "'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "Input: A tinyurl link takes users to a scamming site promising that users can earn thousands of dollars by becoming a Google ( NASDAQ : GOOG ) Cash advertiser .\n",
    "Answer: ''',\n",
    "]\n",
    "\n",
    "# Generate results\n",
    "tokens = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "res = model.generate(**tokens, max_length=512)\n",
    "res_sentences = [tokenizer.decode(i) for i in res]\n",
    "out_text = [o.split(\"Answer: \")[1] for o in res_sentences]\n",
    "\n",
    "# show results\n",
    "for sentiment in out_text:\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from peft import PeftModel  # 0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "\n",
    "\n",
    "template_dict = {\n",
    "    'default': 'Instruction: {instruction}\\nInput: {input}\\nAnswer: '\n",
    "}\n",
    "\n",
    "lora_module_dict = {\n",
    "    'chatglm2': ['query_key_value'],\n",
    "    'falcon': ['query_key_value'],\n",
    "    'bloom': ['query_key_value'],\n",
    "    'internlm': ['q_proj', 'k_proj', 'v_proj'],\n",
    "    'llama2': ['q_proj', 'k_proj', 'v_proj'],\n",
    "    'qwen': [\"c_attn\"],\n",
    "    'mpt': ['Wqkv'],\n",
    "}\n",
    "\n",
    "\n",
    "def get_prompt(template, instruction, input):\n",
    "\n",
    "    if instruction:\n",
    "        return template_dict[template].format(instruction=instruction, input=input)\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "\n",
    "def test_mapping(args, feature):\n",
    "\n",
    "    prompt = get_prompt(\n",
    "        args.instruct_template,\n",
    "        feature['instruction'],\n",
    "        feature['input']\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize(args, tokenizer, feature):\n",
    "\n",
    "    prompt = get_prompt(\n",
    "        args.instruct_template,\n",
    "        feature['instruction'],\n",
    "        feature['input']\n",
    "    )\n",
    "    prompt_ids = tokenizer(\n",
    "        prompt, padding=False,\n",
    "        max_length=args.max_length, truncation=True\n",
    "    )['input_ids']\n",
    "    target_ids = tokenizer(\n",
    "        feature['output'].strip(), padding=False,\n",
    "        max_length=args.max_length, truncation=True,\n",
    "        add_special_tokens=False\n",
    "    )['input_ids']\n",
    "\n",
    "    input_ids = prompt_ids + target_ids\n",
    "    exceed_max_length = len(input_ids) >= args.max_length\n",
    "\n",
    "    # Add EOS Token\n",
    "    if input_ids[-1] != tokenizer.eos_token_id and not exceed_max_length:\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    label_ids = [tokenizer.pad_token_id] * len(prompt_ids) + input_ids[len(prompt_ids):]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": label_ids,\n",
    "        \"exceed_max_length\": exceed_max_length\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_model_name(name, from_remote=False):\n",
    "\n",
    "    if name == 'chatglm2':\n",
    "        return 'THUDM/chatglm2-6b' if from_remote else 'base_models/chatglm2-6b'\n",
    "    elif name == 'llama2':\n",
    "        return 'meta-llama/Llama-2-7b-hf' if from_remote else 'base_models/Llama-2-7b-hf'\n",
    "        # return 'NousResearch/Llama-2-7b-hf' if from_remote else 'base_models/Llama-2-7b-hf-nous'\n",
    "    elif name == 'falcon':\n",
    "        return 'tiiuae/falcon-7b' if from_remote else 'base_models/falcon-7b'\n",
    "    elif name == 'internlm':\n",
    "        return 'internlm/internlm-7b' if from_remote else 'base_models/internlm-7b'\n",
    "    elif name == 'qwen':\n",
    "        return 'Qwen/Qwen-7B' if from_remote else 'base_models/Qwen-7B'\n",
    "    elif name == 'mpt':\n",
    "        return 'cekal/mpt-7b-peft-compatible' if from_remote else 'base_models/mpt-7b-peft-compatible'\n",
    "        # return 'mosaicml/mpt-7b' if from_remote else 'base_models/mpt-7b'\n",
    "    elif name == 'bloom':\n",
    "        return 'bigscience/bloom-7b1' if from_remote else 'base_models/bloom-7b1'\n",
    "    else:\n",
    "        raise ValueError(f\"Undefined base model {name}\")\n",
    "\n",
    "\n",
    "def load_dataset(names, from_remote=False):\n",
    "    dataset_names = [d for d in names.split(',')]\n",
    "    dataset_list = []\n",
    "    for name in dataset_names:\n",
    "        rep = 1\n",
    "        if not os.path.exists(name):\n",
    "            rep = int(name.split('*')[1]) if '*' in name else 1\n",
    "            name = ('FinGPT/fingpt-' if from_remote else 'data/fingpt-') + name.split('*')[0]\n",
    "        tmp_dataset = datasets.load_from_disk(name)\n",
    "        if 'test' not in tmp_dataset:\n",
    "            tmp_dataset = tmp_dataset.train_test_split(0.2, shuffle=True, seed=42)\n",
    "\n",
    "        dataset_list.extend([tmp_dataset] * rep)\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(base_model, peft_model, from_remote=True):\n",
    "\n",
    "    model_name = parse_model_name(base_model, from_remote)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.model_parallel = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if base_model == 'qwen':\n",
    "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<|extra_0|>')\n",
    "    if not tokenizer.pad_token or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, peft_model)\n",
    "    model = model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def test_demo(model, tokenizer):\n",
    "\n",
    "    for task_name, input, instruction in zip(demo_tasks, demo_inputs, demo_instructions):\n",
    "        prompt = 'Instruction: {instruction}\\nInput: {input}\\nAnswer: '.format(\n",
    "            input=input,\n",
    "            instruction=instruction\n",
    "        )\n",
    "        inputs = tokenizer(\n",
    "            prompt, return_tensors='pt',\n",
    "            padding=True, max_length=512,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "        res = model.generate(\n",
    "            **inputs, max_length=512, do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
    "        print(f\"\\n==== {task_name} ====\\n\")\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c2ca3c23e34ca499d60e54859646c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a3ec75c7534567bcebea0220197a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c81dfe3ff94e90ac4e03ef7a4842e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086342f305f640dc81ef3c3764275ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b8a68277084bb79403ec7fce8a2d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8886003c39a4ff2be75a0d4cc50691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b506b550e8f7493a81eee654decc5d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/indi/SNPQuant/youtube_Llama2.ipynb 셀 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m base_model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfalcon\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m peft_model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFinGPT/fingpt-mt_falcon-7b_lora\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m FROM_REMOTE \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfinetuned_models/MT-falcon-linear_202309210126\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model, tokenizer \u001b[39m=\u001b[39m load_model(base_model, peft_model, FROM_REMOTE)\n",
      "\u001b[1;32m/home/indi/SNPQuant/youtube_Llama2.ipynb 셀 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(base_model, peft_model, from_remote\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model_name \u001b[39m=\u001b[39m parse_model_name(base_model, from_remote)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         model_name, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39mmodel_parallel \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/indi/SNPQuant/youtube_Llama2.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/finance-python-GZ7M0BLx/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:511\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 511\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    512\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    513\u001b[0m     )\n\u001b[1;32m    514\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/finance-python-GZ7M0BLx/lib/python3.10/site-packages/transformers/modeling_utils.py:3091\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3082\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3084\u001b[0m     (\n\u001b[1;32m   3085\u001b[0m         model,\n\u001b[1;32m   3086\u001b[0m         missing_keys,\n\u001b[1;32m   3087\u001b[0m         unexpected_keys,\n\u001b[1;32m   3088\u001b[0m         mismatched_keys,\n\u001b[1;32m   3089\u001b[0m         offload_index,\n\u001b[1;32m   3090\u001b[0m         error_msgs,\n\u001b[0;32m-> 3091\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3092\u001b[0m         model,\n\u001b[1;32m   3093\u001b[0m         state_dict,\n\u001b[1;32m   3094\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3095\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3096\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3097\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3098\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3099\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3100\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3101\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3102\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3103\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3104\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3105\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3106\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3107\u001b[0m     )\n\u001b[1;32m   3109\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3110\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/finance-python-GZ7M0BLx/lib/python3.10/site-packages/transformers/modeling_utils.py:3204\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3202\u001b[0m is_safetensors \u001b[39m=\u001b[39m archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3203\u001b[0m \u001b[39mif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_safetensors:\n\u001b[0;32m-> 3204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3205\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3206\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m for them. Alternatively, make sure you have `safetensors` installed if the model you are using\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3207\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m offers the weights in this format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3208\u001b[0m     )\n\u001b[1;32m   3209\u001b[0m \u001b[39mif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3210\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format."
     ]
    }
   ],
   "source": [
    "FROM_REMOTE = True\n",
    "\n",
    "base_model = 'falcon'\n",
    "peft_model = 'FinGPT/fingpt-mt_falcon-7b_lora' if FROM_REMOTE else 'finetuned_models/MT-falcon-linear_202309210126'\n",
    "\n",
    "model, tokenizer = load_model(base_model, peft_model, FROM_REMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
